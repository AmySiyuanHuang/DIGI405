{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lJtK5bHjkEhx"
   },
   "source": [
    "# DIGI405 Lab Class 8: LLM Text classification\n",
    "\n",
    "⚠️ **Run this notebook in Google Colab - not in the DIGI405 JupyterHub**. You will want to change to a GPU/TPU accelerated runtime. To do this go to Runtime > Change runtime type. Read more about GPU/TPU availability in Google Colab [here](https://research.google.com/colaboratory/faq.html#gpu-availability).\n",
    "\n",
    "\n",
    "In this notebook we'll compare the performance of an LLM model, [Hugging Face DistilBERT base (uncased)](https://huggingface.co/distilbert-base-uncased), and a Bag of Words model, the [sci-kit learn multinomial Naive Bayes classifier](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html) on the same dataset.\n",
    "\n",
    "The first part of this notebook is exactly the same as our other lab notebook. Run through all the cells in the notebook to ensure all the data is loaded correctly, and the Naive Bayes model is trained for comparison. You may want to use the 'best' pre-processing and feature selection settings that you found for the Naive Bayes model.\n",
    "\n",
    "In the second part of the notebook we'll fine-tune the [\"distilbert-base-uncased\"](https://huggingface.co/distilbert/distilbert-base-uncased) model on our dataset and compare its classification performance against the Naive Bayes model. DistilBERT base (uncased) is accessed through the 🤗 Hugging Face [transformers](https://huggingface.co/docs/hub/en/transformers) library.\n",
    "\n",
    "---\n",
    "\n",
    "**Remember:** Each time you change settings below, you need to rerun the following cells in order to implement the classification pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NB5SNu01kEh1"
   },
   "source": [
    "## Setup\n",
    "\n",
    "Below we are importing required libraries.\n",
    "\n",
    "We will use the [Naive Bayes Classifier](https://scikit-learn.org/stable/modules/naive_bayes.html). We will also use Scikit-learn's different feature extraction methods based on counts or tf-idf weights. The [NLTK](https://www.nltk.org/) library is used for pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C5Dh1nOTlkrS",
    "outputId": "c4a9181b-9338-42e2-857e-19c598360f96"
   },
   "outputs": [],
   "source": [
    "# Install datasets\n",
    "!pip install datasets==2.14.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EcEI7E9hkEh1"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JkZbetbHkEh1"
   },
   "source": [
    "The following step downloads the following [NLTK](https://www.nltk.org/) resources: stopwords, the POS tagger (used by the NLTK lemmatizer), the Punkt tokenizer models, and the [WordNet lexical database](https://wordnet.princeton.edu/) (used for word meanings and relationships)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iQqVtIXLkEh1",
    "outputId": "e42f0a6f-466d-43c6-a17f-c247aed5786c"
   },
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OvcsIoZtkEh2"
   },
   "source": [
    "This cell loads some defaults for the stop word lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7IozSoaLkEh2"
   },
   "outputs": [],
   "source": [
    "stop_words = None\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as sklearn_stop_words\n",
    "nltk_stop_words = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p7UbgrlmkEh2"
   },
   "source": [
    "Define some functions (you don't need to change anything here, just run the cell) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HApLhOXSkEh2"
   },
   "outputs": [],
   "source": [
    "# nice preview of document\n",
    "def get_preview(docs, targets, target_names, doc_id, max_len=0):\n",
    "    preview = ''\n",
    "    if max_len < 1:\n",
    "        preview += 'Label\\n'\n",
    "        preview += '=====\\n'\n",
    "    else:\n",
    "        preview += str(doc_id)\n",
    "        preview += '\\t'\n",
    "    preview += target_names[targets[doc_id]]\n",
    "    if max_len < 1:\n",
    "        preview += '\\n\\nFull Text\\n'\n",
    "        preview += '=========\\n'\n",
    "        preview += docs[doc_id]\n",
    "        preview += '\\n'\n",
    "    else:\n",
    "        excerpt = get_excerpt(docs[doc_id], max_len)\n",
    "        preview += '\\t' + excerpt\n",
    "    return preview\n",
    "\n",
    "_RE_COMBINE_WHITESPACE = re.compile(r\"\\s+\")\n",
    "\n",
    "# generate an excerpt\n",
    "def get_excerpt(text, max_len):\n",
    "    excerpt = _RE_COMBINE_WHITESPACE.sub(' ',text[0:max_len])\n",
    "    if max_len < len(text):\n",
    "        excerpt += '...'\n",
    "    return excerpt.strip()\n",
    "\n",
    "# combine a defined stop word list (or no stop word list) with any extra stop words defined\n",
    "def set_stop_words(stop_word_list, extra_stop_words):\n",
    "    if len(extra_stop_words) > 0:\n",
    "        if stop_word_list is None:\n",
    "            stop_word_list = []\n",
    "        stop_words = list(stop_word_list) + extra_stop_words\n",
    "    else:\n",
    "        stop_words = stop_word_list\n",
    "\n",
    "    return stop_words\n",
    "\n",
    "# initiate stemming or lemmatising\n",
    "def set_normaliser(normalise):\n",
    "    if normalise == 'PorterStemmer':\n",
    "        normaliser = PorterStemmer()\n",
    "    elif normalise == 'SnowballStemmer':\n",
    "        normaliser = SnowballStemmer('english')\n",
    "    elif normalise == 'WordNetLemmatizer':\n",
    "        normaliser = WordNetLemmatizer()\n",
    "    else:\n",
    "        normaliser = None\n",
    "    return normaliser\n",
    "\n",
    "# we are using a custom tokenisation process to allow different tokenisers and stemming/lemmatising ...\n",
    "def tokenise(doc):\n",
    "    global tokeniser, normalise, normaliser\n",
    "\n",
    "    # you could obviously add more tokenisers here if you wanted ...\n",
    "    if tokeniser == 'sklearn':\n",
    "        tokenizer = RegexpTokenizer(r\"(?u)\\b\\w\\w+\\b\") # this is copied straight from sklearn source\n",
    "        tokens = tokenizer.tokenize(doc)\n",
    "    elif tokeniser == 'word_tokenize':\n",
    "        tokens = word_tokenize(doc)\n",
    "    elif tokeniser == 'wordpunct':\n",
    "        tokens = wordpunct_tokenize(doc)\n",
    "    else:\n",
    "        tokens = word_tokenize(doc)\n",
    "\n",
    "    # if using a normaliser then iterate through tokens and return the normalised tokens ...\n",
    "    if normalise == 'PorterStemmer':\n",
    "        return [normaliser.stem(t) for t in tokens]\n",
    "    elif normalise == 'SnowballStemmer':\n",
    "        return [normaliser.stem(t) for t in tokens]\n",
    "    elif normalise == 'WordNetLemmatizer':\n",
    "        # NLTK's lemmatiser needs parts of speech, otherwise assumes everything is a noun\n",
    "        pos_tokens = nltk.pos_tag(tokens)\n",
    "        lemmatised_tokens = []\n",
    "        for token in pos_tokens:\n",
    "            # NLTK's lemmatiser needs specific values for pos tags - this rewrites them ...\n",
    "            # default to noun\n",
    "            tag = wordnet.NOUN\n",
    "            if token[1].startswith('J'):\n",
    "                tag = wordnet.ADJ\n",
    "            elif token[1].startswith('V'):\n",
    "                tag = wordnet.VERB\n",
    "            elif token[1].startswith('R'):\n",
    "                tag = wordnet.ADV\n",
    "            lemmatised_tokens.append(normaliser.lemmatize(token[0],tag))\n",
    "        return lemmatised_tokens\n",
    "    else:\n",
    "        # no normaliser so just return tokens\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7hJNcm0zkEh3"
   },
   "source": [
    "## Preview stop word lists\n",
    "\n",
    "As discussed in the lecture material, pre-processing can have a major influence on the results of text classification tasks.\n",
    "\n",
    "In particular, you should put thought into whether a stop word list is sensible for your task. The scikit-learn website also makes this point at https://scikit-learn.org/stable/modules/feature_extraction.html#stop-words and recommends caution about using its stop word list! That page also links to a recent paper discussing practical issues with stop word lists, including whether the way you are tokenising your documents matches the tokenisation approach used in your stop word list.\n",
    "\n",
    "Using the cells below you can preview the stop word lists supplied by scikit-learn and NLTK, which we have used previously in class. You will notice they are quite different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5VpO99TokEh4",
    "outputId": "dbabf4f9-bbb7-4a8d-9e2f-d97f64f5495b"
   },
   "outputs": [],
   "source": [
    "print(sklearn_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Udj-pyCLkEh4",
    "outputId": "3735aa8d-ee2f-4166-bc84-aaecd51f799c"
   },
   "outputs": [],
   "source": [
    "print(nltk_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FogXcm6kkEh4"
   },
   "source": [
    "## Load corpus and set train/test split\n",
    "\n",
    "Scikit-learn is packaged with a number of standard data-sets used in machine learning and provides a way to load other data.\n",
    "\n",
    "We will begin by loading texts from two categories in the **[20 newsgroups dataset](http://qwone.com/~jason/20Newsgroups/)** to work through an example classifying documents related to politics and religion.\n",
    "\n",
    "*What is a newsgroup?* We are stretching back into internet history here - way before people talked to strangers on Facebook and X and other social media, there were Usenet Newsgroups! [Here is a link to a Deja News page from 1998](https://web.archive.org/web/19980127204536/http://emarket.dejanews.com/emarket/about/idgs/aboutidgs.shtml) and also a [Wikipedia article](https://en.wikipedia.org/wiki/Usenet_newsgroup) that explains what Newsgroups are all about.\n",
    "\n",
    "This data-set was built from discussions between real people on the internet in the 1990s. Please be aware that within this data-set are texts that include racist, sexist, and other offensive language use.\n",
    "\n",
    "**Note:** This cell also sets the following train/test split: **80% of the data is used for training and 20% is used for testing.** The documents are assigned to each group randomly. It can be useful to rerun this cell to reshuffle your dataset so you can evaluate your model using different data for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sg3F29gQkEh4"
   },
   "outputs": [],
   "source": [
    "# this chooses the categories to load\n",
    "cats = ['talk.politics.misc', 'talk.religion.misc']\n",
    "\n",
    "# this downloads/loads the data\n",
    "# dataset = fetch_20newsgroups(subset='train', categories=cats)\n",
    "dataset = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'), categories=cats)\n",
    "\n",
    "# assign the train/test split - 0.2 is 80% for training, 20% for testing\n",
    "test_size = 0.2\n",
    "\n",
    "# do the train test split ...\n",
    "# docs_train and docs_test are the documents\n",
    "# y_train and y_test are the labels\n",
    "docs_train, docs_test, y_train, y_test = train_test_split(dataset.data, dataset.target,\n",
    "                                                          test_size = test_size, random_state=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WS6qV9dskEh4"
   },
   "source": [
    "## Inspect documents and labels\n",
    "\n",
    "In the next cells we can look at the data we have imported. Firstly, we will preview the document labels and a brief excerpt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KfjPXVqUkEh5",
    "outputId": "54efa0c4-a363-4bb9-e857-7b8478154111"
   },
   "outputs": [],
   "source": [
    "for train_id in range(len(docs_train)):\n",
    "    print(get_preview(docs_train, y_train, dataset.target_names, train_id, max_len=80))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gXwKz-lNkEh5"
   },
   "source": [
    "### You can use the following cell to inspect a specific document and its label based on its index in the training set.\n",
    "\n",
    "Note: The indexes will change each time you import the data above because of the random train/test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "31BSWA-RkEh5",
    "outputId": "64033ebd-3916-4a61-f90d-e1af50afe8c7"
   },
   "outputs": [],
   "source": [
    "train_id = 1  # Enter the index of the document you want to preview\n",
    "print(get_preview(docs_train, y_train, dataset.target_names, train_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4z8oFa79kEh5"
   },
   "source": [
    "## Preprocessing\n",
    "\n",
    "**This next section of the notebook steps you through some key kinds of pre-processing for text classification using Naive Bayes and a bag of words (BoW) model.**\n",
    "\n",
    "On the first run you should read about each setting, but leave the settings as they are. You will come back to this section to tune your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qUVH_0RqkEh5"
   },
   "source": [
    "### Choose between token counts or tf-idf weights\n",
    "\n",
    "You can choose to vectorize your text using frequency or tf-idf weights. Valid values are:\n",
    "```\n",
    "Vectorizer = CountVectorizer\n",
    "```\n",
    "or\n",
    "```\n",
    "Vectorizer = TfidfVectorizer\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b3sIj9EfkEh6"
   },
   "outputs": [],
   "source": [
    "Vectorizer = CountVectorizer  # Set the vectorization method you want to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0mNn2g_akEh6"
   },
   "source": [
    "### Lowercase\n",
    "\n",
    "Setting lowercase to True will transform all document text to lowercase. Setting it to False will not do this transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xqkNNDXKkEh6"
   },
   "outputs": [],
   "source": [
    "lowercase = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HKh0RE5ckEh6"
   },
   "source": [
    "### Set how you are tokenising the text\n",
    "\n",
    "With this notebook you can choose between the following tokenisers.\n",
    "\n",
    "This option duplicates the behaviour of scikit-learn's default tokeniser: \"The default regexp select tokens of 2 or more alphanumeric characters (punctuation is completely ignored and always treated as a token separator)\". In this notebook we duplicate this behaviour using the NLTK's regular expression tokeniser and this regular expression: `r\"(?u)\\b\\w\\w+\\b\"`.\n",
    "```\n",
    "tokeniser = 'sklearn'\n",
    "```\n",
    "You can use this or specify one of the following tokenisers based on NLTK ...\n",
    "\n",
    "Tokenise based on NLTK's wordpunct_tokenize tokeniser (to include words and punctuation!):\n",
    "```\n",
    "tokeniser = 'wordpunct'\n",
    "```\n",
    "This applies NLTK's word_tokenize tokeniser.\n",
    "```\n",
    "tokeniser = 'word_tokenize'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tqBMONlAkEh6"
   },
   "outputs": [],
   "source": [
    "tokeniser = 'sklearn'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PStFPC0_kEh6"
   },
   "source": [
    "### Stemming / Lemmatising\n",
    "\n",
    "This allows to use NLTK stemmers or lemmatisers (or not). Valid options are shown below. Look for more information on the NLTK website: https://www.nltk.org/api/nltk.stem.html. Note: that stemming and lemmatising (in particular) will make the preprocessing take longer!\n",
    "\n",
    "```\n",
    "normalise = None\n",
    "```\n",
    "or\n",
    "```\n",
    "normalise = 'PorterStemmer'\n",
    "```\n",
    "or\n",
    "```\n",
    "normalise = 'SnowballStemmer'\n",
    "```\n",
    "or\n",
    "```\n",
    "normalise = 'WordNetLemmatizer'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rZTh1D5QkEh6"
   },
   "outputs": [],
   "source": [
    "normalise = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cg1tDzJakEh6"
   },
   "source": [
    "### Configure stop words\n",
    "\n",
    "Hopefully you have read the notes on stop word lists above and previewed the different lists.\n",
    "\n",
    "Do you want to apply a stop_word list? Valid values for stop_words below are:\n",
    "```\n",
    "stop_word_list = None\n",
    "```\n",
    "or\n",
    "```\n",
    "stop_word_list = nltk_stop_words\n",
    "```\n",
    "or\n",
    "```\n",
    "stop_word_list = list(sklearn_stop_words)\n",
    "```\n",
    "\n",
    "**Note:** the sklearn_stop_words list is downloaded as a set. We use list() to convert it to a list type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wONFYatIkEh6"
   },
   "outputs": [],
   "source": [
    "stop_word_list = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "71VHU9wmkEh7",
    "outputId": "d1b3e203-8bcc-4fda-d424-7df0c4c6177b"
   },
   "outputs": [],
   "source": [
    "print(stop_word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JLSsYELtkEh7"
   },
   "source": [
    "You can also add extra stop words to any of the lists above.\n",
    "For example:\n",
    "```\n",
    "extra_stop_words = ['stopword1','stopword2','stopword3']\n",
    "```\n",
    "If you don't want extra stop words, then the next cell should look like:\n",
    "```\n",
    "extra_stop_words = []\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mSR0fQg9kEh7"
   },
   "outputs": [],
   "source": [
    "extra_stop_words = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rVkOw42rkEh7"
   },
   "source": [
    "### Filter features based on document frequency\n",
    "\n",
    "The following settings allow you to remove features that occur in many documents or in only a few documents.\n",
    "\n",
    "Firstly, `min_df` ignores terms that occur below a minimum proportion of documents. For example, 0.01 would ignore terms that occur in less than 1% of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XqIOV1ABkEh7"
   },
   "outputs": [],
   "source": [
    "min_df = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0JUZUD1UkEh8"
   },
   "source": [
    "`max_df` allows you to ignore terms above a maximum proportion of documents. For example, 0.95 would ignore terms that occur in more than 95% of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xGQYJxAykEh8"
   },
   "outputs": [],
   "source": [
    "max_df = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JjqK9IInkEh8"
   },
   "source": [
    "### Set a maximum number of features\n",
    "\n",
    "`max_features` set this to `None` for no limit or set to the maximum number of the most frequent features (e.g setting it to 1000 would use the 1000 most frequent features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LimuyNrKkEh8"
   },
   "outputs": [],
   "source": [
    "max_features = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hvjBs8W3kEh8"
   },
   "source": [
    "### Ngrams\n",
    "\n",
    "With ngram_range set to (1,1) you will use unigrams as features i.e. each feature will be a token. If you set it to (1,2) you will use unigrams and bigrams. (1,3) will use unigrams, bigrams and trigrams. If you just want bigrams you would use (2,2). Please note: increasing the ngram range from (1,1) will add more time to preprocessing, as there will be more features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_-hEak-tkEh8"
   },
   "outputs": [],
   "source": [
    "ngram_range = (1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ugVgavVkEh8"
   },
   "source": [
    "### Encoding options\n",
    "\n",
    "You can change the default encoding here and what to do if you get characters outside your default encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9uFxTu7nkEh8"
   },
   "outputs": [],
   "source": [
    "encoding = 'utf-8'\n",
    "decode_error = 'ignore' # what to do if contains characters not of the given encoding - options 'strict', 'ignore', 'replace'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lipNO0WokEh9"
   },
   "source": [
    "## Setup the feature extraction and classification pipeline\n",
    "\n",
    "This sets up a Sci-kit learn pipeline for feature extraction and classification.\n",
    "\n",
    "**Important Note 1:** When you change settings above or reload your dataset you should rerun this cell!\n",
    "\n",
    "**Important Note 2:** This cell outputs the settings you used above, which you can cut and paste into a document to keep track of changes you are making and their effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BF6ITMcckEh9",
    "outputId": "a6ae8861-8983-411f-f207-bf90f7ddcdde"
   },
   "outputs": [],
   "source": [
    "# you shouldn't need to change anything in this cell!\n",
    "\n",
    "stop_words = set_stop_words(stop_word_list, extra_stop_words)\n",
    "normaliser = set_normaliser(normalise)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', Vectorizer(\n",
    "            tokenizer = tokenise,\n",
    "            lowercase = lowercase,\n",
    "            min_df = min_df,\n",
    "            max_df = max_df,\n",
    "            max_features = max_features,\n",
    "            stop_words = stop_words,\n",
    "            ngram_range = ngram_range,\n",
    "            encoding = encoding,\n",
    "            decode_error = decode_error)),\n",
    "    ('classifier', MultinomialNB()), #here is where you would specify an alternative classifier\n",
    "])\n",
    "\n",
    "print('Classifier settings')\n",
    "print('===================')\n",
    "print('classifier:', type(pipeline.steps[1][1]).__name__)\n",
    "print('vectorizer:', type(pipeline.steps[0][1]).__name__)\n",
    "print('classes:', dataset.target_names)\n",
    "print('lowercase:', lowercase)\n",
    "print('tokeniser:', tokeniser)\n",
    "print('normalise:', normalise)\n",
    "print('min_df:', min_df)\n",
    "print('max_df:', max_df)\n",
    "print('max_features:', max_features)\n",
    "if stop_word_list == nltk_stop_words:\n",
    "    print('stop_word_list:', 'nltk_stop_words')\n",
    "elif stop_word_list == list(sklearn_stop_words):\n",
    "    print('stop_word_list:', 'sklearn_stop_words')\n",
    "else:\n",
    "    print('stop_word_list:', 'None')\n",
    "print('extra_stop_words:', extra_stop_words)\n",
    "print('ngram_range:', ngram_range)\n",
    "print('encoding:', encoding)\n",
    "print('decode_error:', decode_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PHI0q9d1kEh9"
   },
   "source": [
    "## Train the classifier and predict labels on test data\n",
    "\n",
    "This cell does the work of training the classifier and predicting labels on test data. It also outputs evaluation metrics, a confusion matrix and features indicative of each class.\n",
    "\n",
    "**Important Note:** You can cut and paste the model output into a document (with the settings above) to keep track of changes you are making and their effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 940
    },
    "id": "gsyIRSPqkEh9",
    "outputId": "0ea1c7eb-6107-4a77-b9e0-ba39d44276d8"
   },
   "outputs": [],
   "source": [
    "# you shouldn't need to change anything in this cell!\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "pipeline.fit(docs_train, y_train)\n",
    "y_predicted = pipeline.predict(docs_test)\n",
    "\n",
    "# print report\n",
    "print('\\nEvaluation metrics')\n",
    "print('==================\\n')\n",
    "print(metrics.classification_report(y_test, y_predicted, target_names = dataset.target_names))\n",
    "cm = metrics.confusion_matrix(y_true=y_test, y_pred=y_predicted, labels=[0, 1])\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=dataset.target_names)\n",
    "disp = disp.plot(include_values=True, cmap='Blues', ax=None, xticks_rotation='vertical')\n",
    "plt.show()\n",
    "\n",
    "vect = pipeline.steps[0][1]\n",
    "clf = pipeline.steps[1][1]\n",
    "\n",
    "print()\n",
    "\n",
    "logodds=clf.feature_log_prob_[1]-clf.feature_log_prob_[0]\n",
    "\n",
    "print(\"Features most indicative of\",dataset.target_names[0])\n",
    "print('============================' + '='*len(dataset.target_names[0]))\n",
    "for i in np.argsort(logodds)[:20]:\n",
    "    print(vect.get_feature_names_out()[i], end=' ')\n",
    "print()\n",
    "print()\n",
    "\n",
    "print(\"Features most indicative of\",dataset.target_names[1])\n",
    "print('============================' + '='*len(dataset.target_names[1]))\n",
    "for i in np.argsort(-logodds)[:20]:\n",
    "    print(vect.get_feature_names_out()[i], end=' ')\n",
    "\n",
    "lookup = dict((v,k) for k,v in vect.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yQssBha4kEh9"
   },
   "source": [
    "## List all features\n",
    "\n",
    "Just for your reference here is a count and list of all features used in this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UYG5OENNkEh-",
    "outputId": "ce1f833f-21e9-48da-c60e-4fc3bc312b60",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Total Features: ',len(vect.get_feature_names_out()))\n",
    "print(vect.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d9x2UqT5kEh-"
   },
   "source": [
    "## Inspect correctly/incorrectly classified documents\n",
    "\n",
    "The output in the next cell is quite long and will take a few moments to generate. It will show you wordclouds and a preview of documents for correctly and incorrectly classified documents. The size of words in the wordclouds are based on adding up counts/tf-idf scores of features based on documents related to each cell in the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "iHAB0WKLkEh-",
    "outputId": "f9f694da-5f06-478a-a2d7-f771948ba1d6"
   },
   "outputs": [],
   "source": [
    "# setup a counter for each cell in the confusion matrix\n",
    "counter = {}\n",
    "previews = {}\n",
    "for true_target, target_name in enumerate(dataset.target_names):\n",
    "    counter[true_target] = {}\n",
    "    previews[true_target] = {}\n",
    "    for predicted_target, target_name in enumerate(dataset.target_names):\n",
    "        counter[true_target][predicted_target] = {}\n",
    "        previews[true_target][predicted_target] = ''\n",
    "\n",
    "# get doc-term matrix for test docs\n",
    "doc_terms = vect.transform(docs_test)\n",
    "\n",
    "# iterate through all predictions, building the counter and preview of docs\n",
    "# there is a better way to do this, but this will do!\n",
    "for doc_id, prediction in enumerate(clf.predict(doc_terms)):\n",
    "    for k, v in enumerate(doc_terms[doc_id].toarray()[0]):\n",
    "        if v > 0:\n",
    "            if lookup[k] not in counter[y_test[doc_id]][prediction]:\n",
    "                counter[y_test[doc_id]][prediction][lookup[k]] = 0\n",
    "            counter[y_test[doc_id]][prediction][lookup[k]] += v\n",
    "\n",
    "    previews[y_test[doc_id]][prediction] += get_preview(docs_test, y_test, dataset.target_names, doc_id, max_len=80) + '\\n'\n",
    "\n",
    "# output a wordcloud and preview of docs for each cell of confusion matrix ...\n",
    "for true_target, target_name in enumerate(dataset.target_names):\n",
    "    for predicted_target, target_name in enumerate(dataset.target_names):\n",
    "        if true_target == predicted_target:\n",
    "            print(f'\\nCORRECTLY CLASSIFIED:\\n{dataset.target_names[true_target]}')\n",
    "        else:\n",
    "            print(f'\\n{dataset.target_names[true_target]} INCORRECTLY CLASSIFIED as: {dataset.target_names[predicted_target]}')\n",
    "        print('=================================================================')\n",
    "\n",
    "        wordcloud = WordCloud(background_color=\"white\", width=800, height=400, color_func=lambda *args, **kwargs: \"black\").generate_from_frequencies(counter[true_target][predicted_target])\n",
    "        plt.figure(figsize=(8, 4), dpi= 150)\n",
    "        plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "        print(previews[true_target][predicted_target])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HbQt-FPdkEh-"
   },
   "source": [
    "## Preview document and its features\n",
    "\n",
    "Use this cell to preview a document using its index in the test set. You can see the predicted label, its actual label, the full text and the features for this specific document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MGqnrMj_kEh-",
    "outputId": "8a791941-6d5d-4803-a3c6-ff52c088d577"
   },
   "outputs": [],
   "source": [
    "test_id = 147\n",
    "\n",
    "print('Prediction')\n",
    "print('==========')\n",
    "print(dataset.target_names[clf.predict(vect.transform([docs_test[test_id]]))[0]])\n",
    "print()\n",
    "\n",
    "print(get_preview(docs_test, y_test, dataset.target_names, test_id))\n",
    "\n",
    "print('Features')\n",
    "print('========')\n",
    "for k, v in enumerate(vect.transform([docs_test[test_id]]).toarray()[0]):\n",
    "    if v > 0:\n",
    "        print(v, '\\t', lookup[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Em7iN6u9lCkz"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8zwJaff96jlA"
   },
   "source": [
    "## DistilBERT\n",
    "\n",
    "**Question:** Run this model and compare the results to the Bag of Words (Naive Bayes) model. Make note of some of the pros and cons of each and discuss with your neighbour or the tutors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f0_pVvd_kEh_"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BZCy2DA0-7-d"
   },
   "source": [
    "The following cell sets-up and trains the model, then returns the results of evaluation against the test set.\n",
    "\n",
    "Just run the cell, there is no need to change anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "lZdojmL0kEh_",
    "outputId": "fe58f617-e9ae-4e66-e8e0-21941dc88da3"
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "print('\\n\\nWe further split our training set into a new training set and an evaluation set\\n\\n')\n",
    "docs_train_b, docs_eval, y_train_b, y_eval = train_test_split(docs_train, y_train, test_size=0.2, random_state=None)\n",
    "\n",
    "# Set up DistilBERT model\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=len(dataset.target_names))\n",
    "\n",
    "train_encodings = tokenizer(docs_train_b, truncation=True, padding=True, max_length=512)\n",
    "eval_encodings = tokenizer(docs_eval, truncation=True, padding=True, max_length=512)\n",
    "\n",
    "train_dataset = Dataset.from_dict({'input_ids': train_encodings['input_ids'], 'attention_mask': train_encodings['attention_mask'], 'labels': y_train_b})\n",
    "eval_dataset = Dataset.from_dict({'input_ids': eval_encodings['input_ids'], 'attention_mask': eval_encodings['attention_mask'], 'labels': y_eval})\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "# Define Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset\n",
    ")\n",
    "\n",
    "print('\\n\\nTraining...please be patient 😉\\n')\n",
    "\n",
    "print(\"\"\"\n",
    "Key points:\n",
    "Convergence: Ideally, both training and validation loss should be decreasing in each epoch and converging to a lower value.\n",
    "Overfitting: If the training loss decreases while the validation loss increases, it indicates that the model is overfitting to the training data.\n",
    "Underfitting: If both training and validation loss remain high, it indicates the model is underfitting - it is not learning well from the data.\n",
    "\"\"\")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "def distilbert_classify(texts, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Classify new text using our fine-tuned DistilBERT model.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # check for cuda availability and set device\n",
    "    model = model.to(device) # move model to device\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=512).to(device) # move inputs to device\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    return probabilities, probabilities.argmax(dim=-1).tolist()\n",
    "\n",
    "print('\\nNow see how the fine-tuned model performs on new data - our original test set')\n",
    "\n",
    "# Evaluate the model\n",
    "probabilities, hf_predictions = distilbert_classify(docs_test, model, tokenizer)\n",
    "\n",
    "print(\"\\nDistilBERT Results:\")\n",
    "print(classification_report(y_test, hf_predictions, target_names=dataset.target_names))\n",
    "\n",
    "# Display confusion matrix\n",
    "cm = metrics.confusion_matrix(y_true=y_test, y_pred=hf_predictions, labels=range(len(dataset.target_names)))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=dataset.target_names)\n",
    "disp.plot(include_values=True, cmap='Blues', xticks_rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "siypopf-p95h",
    "outputId": "7789f3df-504d-4357-fbe7-17e39a318cb3"
   },
   "outputs": [],
   "source": [
    "# Identify and rank incorrectly classified documents\n",
    "incorrect_indices = [i for i, (true, pred) in enumerate(zip(y_test, hf_predictions)) if true != pred]\n",
    "incorrect_docs = [\n",
    "    (\n",
    "        i,  # Original index\n",
    "        docs_test[i],\n",
    "        dataset.target_names[y_test[i]],  # Map true label to text\n",
    "        dataset.target_names[hf_predictions[i]],  # Map predicted label to text\n",
    "        probabilities[i].max().item()\n",
    "    )\n",
    "    for i in incorrect_indices\n",
    "]\n",
    "\n",
    "# Sort by the highest confidence in the incorrect class\n",
    "incorrect_docs_sorted = sorted(incorrect_docs, key=lambda x: x[4], reverse=True)\n",
    "df_incorrect = pd.DataFrame(incorrect_docs_sorted, columns=['Test set index', 'Document', 'True Label', 'Predicted Label', 'Confidence'])\n",
    "\n",
    "print(\"\\nIncorrectly Classified Documents\\n\")\n",
    "print(\"Confidence is the probability the model has assigned for the given document belonging to the predicted class.\\n\")\n",
    "display(df_incorrect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9HB9NpIV-YLT",
    "outputId": "fcdd602f-fdca-4af6-b364-a2aa04a35c53"
   },
   "outputs": [],
   "source": [
    "index_to_display = 23\n",
    "print(df_incorrect.loc[index_to_display, 'Document'])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
